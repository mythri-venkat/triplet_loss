{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import common\n",
    "import loss\n",
    "from models import Trinet\n",
    "\n",
    "#import ipdb\n",
    "\n",
    "parser = ArgumentParser(description='Train a triplet loss person re-identification network.')\n",
    "\n",
    "# Required arguments\n",
    "parser.add_argument(\n",
    "    '--experiment_root', default=\"./marketroot\",\n",
    "    help='Location used to store checkpoints and dumped data.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--train_set',default=\"data/market1501_train.csv\",\n",
    "    help='Path to the train_set csv file.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--image_root', type=common.readable_directory,default=\"../Market-1501-v15.09.15\",\n",
    "    help='Path that will be pre-pended to the filenames in the train_set csv')\n",
    "\n",
    "# Optional with defaults.\n",
    "parser.add_argument(\n",
    "    '--resume', action='store_true', default=False,\n",
    "    help='With this flag, all other arguments apart from the experiment_root'\n",
    "         'are ignored and a previously saved set of arguments is loaded.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--embedding_dim', default=128, type=common.positive_int,\n",
    "    help='Dimensionality of the embedding space.')\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\n",
    "    '--batch_p', default=32, type=common.positive_int,\n",
    "    help='The number P used in the PK-batches')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--batch_k', default=4, type=common.positive_int,\n",
    "    help='The number K used in PK-batches')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--net_input_height', default=256, type=common.positive_int,\n",
    "    help='Height of the input directly fed into the network.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--net_input_width', default=128, type=common.positive_int,\n",
    "    help='Width of the input directly fed into the network.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--learning_rate', default=3e-4, type=common.positive_float,\n",
    "    help='The initial value of the learning-rate, before it kicks in.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--train_iterations', default=25000, type=common.positive_int,\n",
    "    help='Number of training iterations.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--decay_start_iteration', default=15000, type=int,\n",
    "    help='At which iteration the learning-rate decay should kick-in.'\n",
    "         'Set to -1 to disable decay completely.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--checkpoint_frequency', default=1000, type=common.nonnegative_int,\n",
    "    help='After how many iterations a checkpoint is stored. Set this to 0 to '\n",
    "         'disable intermediate storing. This will result in only one final '\n",
    "         'checkpoint.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--loading_threads', default=8, type=common.positive_int,\n",
    "    help='Number of threads used for parallel loading.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--margin', default='soft', type=common.float_or_string,\n",
    "    help='What margin to use: a float value for hard-margin, \"soft\" for '\n",
    "         'soft-margin, or no margin if \"none\".')\n",
    "\n",
    "def show_all_parameters( args):\n",
    "    print('Training using the following parameters:')\n",
    "    for key, value in sorted(vars(args).items()):\n",
    "        print('{}: {}'.format(key, value))\n",
    "\n",
    "\n",
    "def sample_k_fids_for_pid(pid, all_fids, all_pids, batch_k):\n",
    "    \"\"\" Given a PID, select K FIDs of that specific PID. \"\"\"\n",
    "    #ipdb.set_trace()\n",
    "    possible_fids = tf.boolean_mask(all_fids, tf.equal(all_pids, pid))\n",
    "\n",
    "    # The following simply used a subset of K of the possible FIDs\n",
    "    # if >= K are available. Otherwise, we first create a padded list\n",
    "    # of indices which contain a multiple of the original FID count such\n",
    "    # that all of them will be sampled equally likely.\n",
    "    count = tf.shape(possible_fids)[0]\n",
    "    padded_count = tf.cast(tf.math.ceil(batch_k / tf.cast(count, tf.float32)), tf.int32) * count\n",
    "    full_range = tf.math.mod(tf.range(padded_count), count)\n",
    "\n",
    "    shuffled = tf.random.shuffle(full_range)\n",
    "    selected_fids = tf.gather(possible_fids, shuffled[:batch_k])\n",
    "    return selected_fids, tf.fill([batch_k], pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using the following parameters:\n",
      "batch_k: 4\n",
      "batch_p: 32\n",
      "checkpoint_frequency: 1000\n",
      "decay_start_iteration: 15000\n",
      "embedding_dim: 128\n",
      "experiment_root: ./marketroot\n",
      "image_root: ../Market-1501-v15.09.15\n",
      "learning_rate: 0.0003\n",
      "loading_threads: 8\n",
      "margin: soft\n",
      "net_input_height: 256\n",
      "net_input_width: 128\n",
      "resume: False\n",
      "train_iterations: 25000\n",
      "train_set: data/market1501_train.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# my_devices = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
    "# tf.config.experimental.set_visible_devices(devices= my_devices, device_type='CPU')\n",
    "\n",
    "    # To find out which devices your operations and tensors are assigned to\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "\n",
    "show_all_parameters( args)\n",
    "\n",
    "if not args.train_set:\n",
    "    parser.print_help()\n",
    "    print(\"You didn't specify the 'train_set' argument!\")\n",
    "    sys.exit(1)\n",
    "if not args.image_root:\n",
    "    parser.print_help()\n",
    "    print(\"You didn't specify the 'image_root' argument!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "pids, fids = common.load_dataset(args.train_set, args.image_root)\n",
    "\n",
    "unique_pids = np.unique(pids)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(unique_pids)\n",
    "dataset = dataset.shuffle(len(unique_pids))\n",
    "\n",
    "# Take the dataset size equal to a multiple of the batch-size, so that\n",
    "# we don't get overlap at the end of each epoch.\n",
    "dataset = dataset.take((len(unique_pids) // args.batch_p) * args.batch_p)\n",
    "dataset = dataset.repeat(None)    # Repeat indefinitely.\n",
    "\n",
    "# For every PID, get K images.\n",
    "dataset = dataset.map(lambda pid: sample_k_fids_for_pid(\n",
    "    pid, all_fids=fids, all_pids=pids, batch_k=args.batch_k))\n",
    "\n",
    "# Ungroup/flatten the batches\n",
    "dataset = dataset.unbatch()\n",
    "\n",
    "# Convert filenames to actual image tensors.\n",
    "net_input_size = (args.net_input_height, args.net_input_width)\n",
    "dataset = dataset.map(lambda fid, pid: common.fid_to_image(\n",
    "                      fid, pid, image_root=args.image_root,\n",
    "                      image_size=net_input_size)\n",
    "                      )\n",
    "\n",
    "# Group the data into PK batches.\n",
    "batch_size = args.batch_p * args.batch_k\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "dataset = dataset.prefetch(1)\n",
    "dataiter = iter(dataset)\n",
    "print(\"1\")\n",
    "model = Trinet(args.embedding_dim)\n",
    "print(\"model done\")\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(args.learning_rate,args.train_iterations - args.decay_start_iteration, 0.001)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "writer = tf.summary.create_file_writer(args.experiment_root)\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(0), optimizer=optimizer, net=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, args.experiment_root, max_to_keep=10)\n",
    "\n",
    "if args.resume:\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "\n",
    "for epoch in range(args.train_iterations):\n",
    "\n",
    "    # for images,fids,pids in dataset:\n",
    "    images,fids,pids = next(dataiter)\n",
    "    with tf.GradientTape() as tape:\n",
    "        emb = model(images)\n",
    "        dists = loss.cdist(emb,emb)\n",
    "        losses,top1,prec,topksame,negdist,posdist = loss.batch_hard(dists,pids,args.margin,args.batch_k)\n",
    "        lossavg = tf.reduce_mean(losses)\n",
    "        lossnp = losses.numpy()\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar(\"loss\",lossavg,step=epoch)\n",
    "        tf.summary.scalar('batch_top1', top1,step=epoch)\n",
    "        tf.summary.scalar('batch_prec_at_{}'.format(args.batch_k-1), prec,step=epoch)\n",
    "        tf.summary.histogram('losses',losses,step=epoch)\n",
    "        tf.summary.histogram('embedding_dists', dists,step=epoch)\n",
    "        tf.summary.histogram('embedding_pos_dists', negdist,step=epoch)\n",
    "        tf.summary.histogram('embedding_neg_dists', posdist,step=epoch)\n",
    "\n",
    "    print('iter:{:6d}, loss min|avg|max: {:.3f}|{:.3f}|{:6.3f}, '\n",
    "            ' batch-p@{}: {:.2%}'.format(\n",
    "                epoch,\n",
    "                float(np.min(lossnp)),\n",
    "                float(np.mean(lossnp)),\n",
    "                float(np.max(lossnp)),\n",
    "                args.batch_k-1, float(prec)))\n",
    "    grad = tape.gradient(lossavg,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grad,model.trainable_variables))\n",
    "    ckpt.step.assign_add(1)\n",
    "    if epoch%args.checkpoint_frequency == 0:\n",
    "        manager.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ventf2",
   "language": "python",
   "name": "ventf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
